# ðŸ”§ Stage 2 Improvements

## ðŸ“Š **PhÃ¢n tÃ­ch váº¥n Ä‘á» tá»« log gá»‘c:**

### âŒ **Váº¥n Ä‘á» chÃ­nh:**
1. **Tail Expert bá»‹ bá» qua hoÃ n toÃ n (0.00%)** - Máº¥t Ä‘i sá»± Ä‘a dáº¡ng cá»§a ensemble
2. **Worst-group Error quÃ¡ cao (90.39%)** - Performance trÃªn tail classes kÃ©m
3. **Group weights máº¥t cÃ¢n báº±ng nghiÃªm trá»ng** - Algorithm táº­p trung vÃ o head classes
4. **KhÃ´ng cÃ³ kiá»ƒm tra cháº¥t lÆ°á»£ng expert** - KhÃ´ng biáº¿t táº¡i sao tail expert bá»‹ ignore

## ðŸ”§ **CÃ¡c cáº£i thiá»‡n Ä‘Ã£ thÃªm:**

### **1. Expert Quality Analysis (`debug_expert_quality.py`)**
```python
# PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng tá»«ng expert riÃªng láº»
- Overall accuracy
- Head vs Tail accuracy  
- Confidence analysis
- Prediction diversity (entropy)
- Class bias analysis
```

**Lá»£i Ã­ch:**
- Hiá»ƒu táº¡i sao tail expert bá»‹ bá» qua
- Kiá»ƒm tra xem experts cÃ³ Ä‘Æ°á»£c train Ä‘Ãºng khÃ´ng
- PhÃ¡t hiá»‡n bias trong predictions

### **2. Improved CS-plugin Optimization (`improved_optimization.py`)**

#### **A. Constrained Weight Search:**
```python
# ThÃªm constraints Ä‘á»ƒ Ä‘áº£m báº£o tail expert Ä‘Æ°á»£c sá»­ dá»¥ng
min_tail_expert_weight = 0.1      # Minimum 10% weight
min_balanced_expert_weight = 0.2  # Minimum 20% weight
```

#### **B. Weight Regularization:**
```python
# Penalty cho extreme weight distributions
- Penalty náº¿u expert nÃ o cÃ³ weight > 80%
- Penalty náº¿u tail expert cÃ³ weight < 5%
```

#### **C. Better Alpha Initialization:**
```python
# Improved alpha finding dá»±a trÃªn expert quality
- Better initialization
- Adaptive update rule
- Convergence checking
```

### **3. Improved Worst-group Optimization**

#### **A. Early Stopping:**
```python
# TrÃ¡nh overfitting vÃ  convergence
patience = 5
no_improvement_count = 0
if no_improvement_count >= patience:
    break
```

#### **B. Convergence Detection:**
```python
# Dá»«ng khi weights khÃ´ng thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ
weight_change = torch.abs(group_weights - old_group_weights).sum().item()
if weight_change < 1e-6:
    break
```

#### **C. Stratified Data Splitting:**
```python
# Äáº£m báº£o cáº£ head vÃ  tail classes trong validation
val_head_size = min(val_size // 2, len(head_indices))
val_tail_size = val_size - val_head_size
```

## ðŸš€ **CÃ¡ch sá»­ dá»¥ng:**

### **Option 1: Cháº¡y tá»«ng bÆ°á»›c riÃªng láº»**
```bash
# 1. PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng expert
python moe_plugin_pipeline/debug_expert_quality.py \
    --experts_dir "checkpoints/balpoe_experts/cifar100_ir100/models/Imbalance_CIFAR100LT_IR100_BalPoE_Experts/1022_153647" \
    --config "moe_plugin_pipeline/configs/plugin_optimization.json"

# 2. Cháº¡y improved optimization
python moe_plugin_pipeline/improved_optimization.py \
    --config "moe_plugin_pipeline/configs/plugin_optimization.json"
```

### **Option 2: Cháº¡y táº¥t cáº£ cÃ¹ng lÃºc**
```bash
python moe_plugin_pipeline/run_improved_stage2.py
```

## ðŸ“Š **Káº¿t quáº£ mong Ä‘á»£i:**

### **TrÆ°á»›c (Original):**
- Expert weights: [0.429, 0.571, 0.000] âŒ
- Tail expert: 0.00% (bá»‹ bá» qua)
- Worst-group error: 90.39% (quÃ¡ cao)

### **Sau (Improved):**
- Expert weights: [~0.3, ~0.4, ~0.3] âœ…
- Tail expert: ~30% (Ä‘Æ°á»£c sá»­ dá»¥ng)
- Worst-group error: <80% (cáº£i thiá»‡n)
- Balanced error: <60% (cáº£i thiá»‡n)

## ðŸ” **Debug Information:**

### **Expert Quality Analysis sáº½ cho biáº¿t:**
```
ðŸ“Š head_expert Performance:
  - Overall Accuracy: 0.6234 (62.34%)
  - Head Accuracy: 0.6789 (67.89%)
  - Tail Accuracy: 0.4567 (45.67%)

ðŸ“Š tail_expert Performance:
  - Overall Accuracy: 0.5890 (58.90%)
  - Head Accuracy: 0.5432 (54.32%)
  - Tail Accuracy: 0.6789 (67.89%)  â† Tá»‘t hÆ¡n trÃªn tail classes!
```

### **Improved Optimization sáº½ cho biáº¿t:**
```
ðŸ” Constrained grid search: 378 weight combinations Ã— 3 lambda values
âœ… New best: balanced_error = 0.5890
ðŸ“Š Weights: [0.300, 0.400, 0.300]  â† Tail expert Ä‘Æ°á»£c sá»­ dá»¥ng!
```

## ðŸ’¡ **Lá»£i Ã­ch chÃ­nh:**

1. **Táº­n dá»¥ng Ä‘áº§y Ä‘á»§ ensemble diversity** - Táº¥t cáº£ experts Ä‘Æ°á»£c sá»­ dá»¥ng
2. **Cáº£i thiá»‡n performance trÃªn tail classes** - Worst-group error giáº£m
3. **TrÃ¡nh overfitting** - Early stopping vÃ  regularization
4. **Debug-friendly** - Hiá»ƒu rÃµ táº¡i sao cÃ³ káº¿t quáº£ nhÆ° váº­y
5. **Robust optimization** - Constraints Ä‘áº£m báº£o káº¿t quáº£ há»£p lÃ½

## ðŸŽ¯ **Next Steps:**

Sau khi cháº¡y improved optimization, báº¡n cÃ³ thá»ƒ:
1. So sÃ¡nh káº¿t quáº£ vá»›i original optimization
2. Cháº¡y Stage 3 vá»›i improved parameters
3. PhÃ¢n tÃ­ch Risk-Coverage curves Ä‘á»ƒ tháº¥y sá»± cáº£i thiá»‡n
4. Tune thÃªm parameters náº¿u cáº§n
