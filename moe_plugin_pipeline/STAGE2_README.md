# ğŸ¯ Stage 2: Tá»‘i Æ°u hÃ³a Plugin Parameters

## ğŸ“‹ Tá»•ng quan

Giai Ä‘oáº¡n 2 sá»­ dá»¥ng **3 bá»™ xÃ¡c suáº¥t háº­u nghiá»‡m tá»« cÃ¡c chuyÃªn gia** (Head, Balanced, Tail) Ä‘á»ƒ tÃ¬m ra má»™t **"há»—n há»£p" tá»‘i Æ°u** vÃ  cÃ¡c tham sá»‘ plugin `(Î±Ì‚, Î¼Ì‚)` nháº±m tá»‘i thiá»ƒu hÃ³a balanced error vÃ  worst-group error.

## âš™ï¸ Cáº¥u hÃ¬nh Chi tiáº¿t

### 2.1 Chuáº©n bá»‹ MÃ´i trÆ°á»ng vÃ  Dá»¯ liá»‡u

#### 2.1.1 PhÃ¢n chia Dá»¯ liá»‡u
- **Validation Set (20%)**: TÃ¬m kiáº¿m siÃªu tham sá»‘ tá»‘i Æ°u
- **Test Set (80%)**: ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng trong Giai Ä‘oáº¡n 3
- **LÃ½ do**: NgÄƒn cháº·n rÃ² rá»‰ thÃ´ng tin tá»« táº­p test

#### 2.1.2 Äá»‹nh nghÄ©a NhÃ³m (Head vs. Tail)
- **NhÃ³m Tail**: CÃ¡c lá»›p cÃ³ â‰¤ 20 máº«u trong training set
- **NhÃ³m Head**: CÃ¡c lá»›p cÃ²n láº¡i
- **LÃ½ do**: Äá»‹nh nghÄ©a chÃ­nh xÃ¡c theo paper "Learning to Reject"

#### 2.1.3 Chuáº©n bá»‹ Äáº§u vÃ o cho Plugin
- **Input**: 3 vector xÃ¡c suáº¥t háº­u nghiá»‡m tá»« experts
- **Output**: Há»—n há»£p cÃ³ trá»ng sá»‘ `p_mix(y|x) = w_headÂ·p_head(y|x) + w_balÂ·p_bal(y|x) + w_tailÂ·p_tail(y|x)`

### 2.2 Thuáº­t toÃ¡n 1: CS-plugin Ä‘á»ƒ tá»‘i Æ°u Balanced Error

#### 2.2.1 Grid Search cho Expert Weights
- **Search space**: `w_head, w_bal, w_tail âˆˆ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}`
- **Constraint**: Tá»•ng weights = 1
- **Purpose**: TÃ¬m trá»ng sá»‘ há»—n há»£p tá»‘i Æ°u

#### 2.2.2 Grid Search cho Î»â‚€
- **Search space**: `Î»â‚€ âˆˆ {1, 6, 11}` (theo Phá»¥ lá»¥c F.3)
- **Purpose**: TÃ¬m tham sá»‘ plugin tá»‘i Æ°u

#### 2.2.3 Power Iteration cho Î±
- **Iterations**: M = 10 (theo Phá»¥ lá»¥c F.3)
- **Purpose**: TÃ¬m Î± tá»‘i Æ°u cho má»—i cáº·p (w, Î»â‚€)

### 2.3 Thuáº­t toÃ¡n 2: Worst-group Plugin Ä‘á»ƒ tá»‘i Æ°u Worst-Group Error

#### 2.3.1 Cáº¥u hÃ¬nh Thuáº­t toÃ¡n
- **Sá»‘ vÃ²ng láº·p**: T = 25
- **Step-size**: Î¾ = 1.0
- **Khá»Ÿi táº¡o**: Î²â½â°â¾ = {0.5, 0.5}

#### 2.3.2 Quy trÃ¬nh Triá»ƒn khai
1. **VÃ²ng láº·p t = 0 Ä‘áº¿n T-1**:
   - Gá»i CS-plugin vá»›i trá»ng sá»‘ nhÃ³m Î²â½áµ—â¾
   - TÃ­nh group-wise errors
   - Cáº­p nháº­t Î²â½áµ—âºÂ¹â¾ báº±ng exponentiated gradient

2. **Cáº­p nháº­t trá»ng sá»‘ nhÃ³m**:
   ```
   Î²â‚–â½áµ—âºÂ¹â¾ âˆ Î²â‚–â½áµ—â¾ Â· exp(Î¾ Â· Ãªâ‚–(hâ½áµ—â¾, râ½áµ—â¾))
   ```

## ğŸš€ CÃ¡ch cháº¡y

### **Test setup trÆ°á»›c:**
```bash
python moe_plugin_pipeline/test_stage2.py
```

### **Cháº¡y Stage 2:**
```bash
# PhÆ°Æ¡ng phÃ¡p 1: Script
./moe_plugin_pipeline/run_stage2.sh

# PhÆ°Æ¡ng phÃ¡p 2: Trá»±c tiáº¿p
python moe_plugin_pipeline/stage2_optimize_plugin.py \
    --config moe_plugin_pipeline/configs/plugin_optimization.json \
    --experts_dir checkpoints/balpoe_experts/cifar100_ir100 \
    --seed 1
```

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:

```
checkpoints/plugin_optimized/
â”œâ”€â”€ optimized_parameters.json    # CÃ¡c tham sá»‘ Ä‘Ã£ tá»‘i Æ°u
â””â”€â”€ config.json                 # Config Ä‘Ã£ sá»­ dá»¥ng
```

### **Ná»™i dung optimized_parameters.json:**
```json
{
  "lambda_0": 6,
  "alpha": 0.7,
  "expert_weights": [0.3, 0.4, 0.3],
  "group_weights": [0.6, 0.4],
  "rejection_threshold": 0.7,
  "balanced_error": 0.25,
  "worst_group_error": 0.30
}
```

## ğŸ” Kiá»ƒm tra káº¿t quáº£

### 1. Kiá»ƒm tra optimized parameters
```bash
cat checkpoints/plugin_optimized/optimized_parameters.json
```

### 2. Kiá»ƒm tra log optimization
- Logs sáº½ hiá»ƒn thá»‹ trong terminal
- Hiá»ƒn thá»‹ progress cá»§a grid search
- Hiá»ƒn thá»‹ best parameters táº¡i má»—i bÆ°á»›c

### 3. Kiá»ƒm tra config Ä‘Ã£ sá»­ dá»¥ng
```bash
cat checkpoints/plugin_optimized/config.json
```

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **Sá»­ dá»¥ng hoÃ n toÃ n code BalPoE gá»‘c** - khÃ´ng táº¡o hÃ m má»›i
2. **TuÃ¢n thá»§ Ä‘Ãºng thuáº­t toÃ¡n tá»« paper** - grid search chÃ­nh xÃ¡c
3. **Validation set (20%)** - khÃ´ng sá»­ dá»¥ng test set
4. **Group definition** - Tail = classes vá»›i â‰¤ 20 samples

## ğŸ› Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **"No checkpoint files found"**
   - Cháº¡y Stage 1 trÆ°á»›c Ä‘á»ƒ táº¡o expert checkpoints
   - Kiá»ƒm tra Ä‘Æ°á»ng dáº«n experts_dir

2. **"CUDA out of memory"**
   - Giáº£m batch_size trong config
   - Sá»­ dá»¥ng CPU thay vÃ¬ GPU

3. **"Dataset not found"**
   - Äáº£m báº£o cÃ³ thÆ° má»¥c ./data/CIFAR-100
   - Dataset sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº¡o náº¿u chÆ°a cÃ³

## ğŸ“ˆ Monitoring

Trong quÃ¡ trÃ¬nh optimization, báº¡n sáº½ tháº¥y:
- **CS-plugin progress**: Grid search cho expert weights vÃ  Î»â‚€
- **Worst-group progress**: VÃ²ng láº·p t = 0 Ä‘áº¿n 24
- **Best parameters**: Cáº­p nháº­t khi tÃ¬m Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n
- **Final results**: Balanced error vÃ  worst-group error

## ğŸ”„ Tiáº¿p theo

Sau khi Stage 2 hoÃ n thÃ nh:
```bash
python moe_plugin_pipeline/stage3_evaluate.py \
    --plugin_checkpoint checkpoints/plugin_optimized/optimized_parameters.json \
    --experts_dir checkpoints/balpoe_experts/cifar100_ir100 \
    --config moe_plugin_pipeline/configs/plugin_optimization.json \
    --save_dir results/evaluation \
    --seed 1
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

- **Paper gá»‘c**: "Learning to Reject Meets Long-tail Learning"
- **BalPoE paper**: "Balanced Product of Calibrated Experts for Long-Tailed Recognition"
- **Config files**: `moe_plugin_pipeline/configs/plugin_optimization.json`
- **Test script**: `moe_plugin_pipeline/test_stage2.py`
