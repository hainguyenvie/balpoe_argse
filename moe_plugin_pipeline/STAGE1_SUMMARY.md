# ğŸ“‹ Stage 1 Summary: BalPoE Experts Training

## âœ… ÄÃ£ hoÃ n thÃ nh

### 1. **Config File** (`configs/cifar100_ir100_balpoe.json`)
- âœ… **Dataset**: CIFAR-100-LT vá»›i IR=100 (imb_factor=0.01)
- âœ… **Architecture**: ResNet-32 vá»›i 3 experts
- âœ… **Optimizer**: SGD (lr=0.1, momentum=0.9, weight_decay=5e-4)
- âœ… **LR Scheduler**: CustomLR (step1=160, step2=180, gamma=0.1)
- âœ… **Epochs**: 200 (Standard training)
- âœ… **Batch Size**: 128
- âœ… **Experts**: tau_list=[0, 1.0, 2.0] (Î» âˆˆ {1, 0, -1})
- âœ… **Mixup**: alpha=0.4 (calibration báº¯t buá»™c)
- âœ… **Add extra info**: true (Ä‘á»ƒ láº¥y expert predictions)

### 2. **Training Script** (`stage1_train_balpoe.py`)
- âœ… **Sá»­ dá»¥ng hoÃ n toÃ n code BalPoE gá»‘c**
- âœ… **KhÃ´ng táº¡o hÃ m má»›i** - chá»‰ import vÃ  sá»­ dá»¥ng
- âœ… **TÆ°Æ¡ng thÃ­ch vá»›i train.py gá»‘c**
- âœ… **Support táº¥t cáº£ CLI options cá»§a BalPoE**

### 3. **Helper Scripts**
- âœ… **run_stage1.sh**: Script cháº¡y Ä‘Æ¡n giáº£n
- âœ… **test_stage1.py**: Kiá»ƒm tra setup
- âœ… **STAGE1_GUIDE.md**: HÆ°á»›ng dáº«n chi tiáº¿t

## ğŸ” Kiá»ƒm tra vá»›i yÃªu cáº§u cá»§a báº¡n

### âœ… **1.1 Environment & Data**
- âœ… Dataset: CIFAR-100-LT, IR=100
- âœ… imb_factor: 0.01 (tÆ°Æ¡ng Ä‘Æ°Æ¡ng IR=100)
- âœ… Architecture: ResNet-32

### âœ… **1.2 Training Hyperparameters**
- âœ… Optimizer: SGD vá»›i lr=0.1, momentum=0.9, weight_decay=5e-4
- âœ… LR Scheduler: Multi-step (160, 180 epochs, gamma=0.1)
- âœ… Epochs: 200
- âœ… Batch Size: 128

### âœ… **1.3 Expert Configuration**
- âœ… Sá»‘ experts: 3
- âœ… Tau values: [0, 1.0, 2.0] (tÆ°Æ¡ng Ä‘Æ°Æ¡ng Î» âˆˆ {1, 0, -1})
- âœ… Expert 1 (Ï„=0, Î»=1): Head expert - Cross-Entropy
- âœ… Expert 2 (Ï„=1, Î»=0): Balanced expert - Balanced Softmax
- âœ… Expert 3 (Ï„=2, Î»=-1): Tail expert - Inverse distribution

### âœ… **1.4 Calibration**
- âœ… Method: Mixup
- âœ… Alpha: 0.4 (tá»‘i Æ°u cho CIFAR-100-LT)
- âœ… Target mix strategy: mix_input

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### **Test setup trÆ°á»›c:**
```bash
python moe_plugin_pipeline/test_stage1.py
```

### **Cháº¡y Stage 1:**
```bash
# PhÆ°Æ¡ng phÃ¡p 1: Script
./moe_plugin_pipeline/run_stage1.sh

# PhÆ°Æ¡ng phÃ¡p 2: Trá»±c tiáº¿p
python moe_plugin_pipeline/stage1_train_balpoe.py \
    -c moe_plugin_pipeline/configs/cifar100_ir100_balpoe.json \
    -s 1

# PhÆ°Æ¡ng phÃ¡p 3: Sá»­ dá»¥ng train.py gá»‘c
python train.py -c moe_plugin_pipeline/configs/cifar100_ir100_balpoe.json -s 1
```

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:
```
checkpoints/balpoe_experts/cifar100_ir100/
â”œâ”€â”€ config.json
â”œâ”€â”€ checkpoint-epoch50.pth
â”œâ”€â”€ checkpoint-epoch100.pth
â”œâ”€â”€ checkpoint-epoch150.pth
â”œâ”€â”€ checkpoint-epoch200.pth
â””â”€â”€ model_best.pth
```

## ğŸ”§ KhÃ´ng cáº§n thÃªm hÃ m má»›i

Táº¥t cáº£ cÃ¡c hÃ m cáº§n thiáº¿t Ä‘Ã£ cÃ³ sáºµn trong BalPoE gá»‘c:
- âœ… `parse_tau_list()` - utils/util.py:282
- âœ… `learning_rate_scheduler()` - utils/util.py:331
- âœ… `write_json()` - utils/util.py:76
- âœ… `seed_everything()` - utils/util.py:322
- âœ… `ConfigParser` - parse_config.py:11
- âœ… `Trainer` - trainer/trainer.py
- âœ… `BSExpertLoss` - model/loss.py:8
- âœ… `Combiner` - utils/combiner.py:5

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **Cháº¡y tá»« thÆ° má»¥c BalPoE gá»‘c** - khÃ´ng pháº£i tá»« moe_plugin_pipeline
2. **Calibration báº¯t buá»™c** - Mixup vá»›i Î±=0.4 lÃ  yÃªu cáº§u lÃ½ thuyáº¿t
3. **Expert diversity** - 3 experts táº¡o sá»± Ä‘a dáº¡ng tá»‘i Ä‘a
4. **Standard training** - 200 epochs Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng

## ğŸ¯ Tiáº¿p theo

Sau khi Stage 1 hoÃ n thÃ nh:
```bash
python moe_plugin_pipeline/stage2_optimize_plugin.py \
    --experts_dir checkpoints/balpoe_experts/cifar100_ir100 \
    --config moe_plugin_pipeline/configs/plugin_optimization.json
```
