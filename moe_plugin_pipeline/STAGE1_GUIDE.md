# ğŸ¯ Giai Ä‘oáº¡n 1: Huáº¥n luyá»‡n BalPoE Experts - HÆ°á»›ng dáº«n Chi tiáº¿t

## ğŸ“‹ Tá»•ng quan

Giai Ä‘oáº¡n 1 táº¡o ra **3 chuyÃªn gia ResNet-32 Ä‘a dáº¡ng vÃ  Ä‘Æ°á»£c hiá»‡u chá»‰nh tá»‘t** lÃ m Ä‘áº§u vÃ o cho cÃ¡c thuáº­t toÃ¡n plugin. Sá»­ dá»¥ng hoÃ n toÃ n code tá»« BalPoE gá»‘c, khÃ´ng táº¡o hÃ m má»›i.

## âš™ï¸ Cáº¥u hÃ¬nh Chi tiáº¿t

### 1.1 Dataset & Environment
- **Dataset**: CIFAR-100-LT vá»›i IR=100 (imb_factor=0.01)
- **Architecture**: ResNet-32 vá»›i 3 experts
- **Device**: CUDA (náº¿u cÃ³) hoáº·c CPU

### 1.2 Training Hyperparameters (Standard Training)
- **Optimizer**: SGD
  - Learning Rate: 0.1
  - Momentum: 0.9
  - Weight Decay: 5e-4
  - Nesterov: true
- **LR Scheduler**: CustomLR (Multi-step)
  - Step 1: 160 epochs
  - Step 2: 180 epochs
  - Gamma: 0.1
  - Warmup: 5 epochs
- **Epochs**: 200
- **Batch Size**: 128

### 1.3 Expert Configuration (TrÃ¡i tim cá»§a BalPoE)
- **Sá»‘ experts**: 3
- **Tau values**: [0, 1.0, 2.0] (tÆ°Æ¡ng Ä‘Æ°Æ¡ng Î» âˆˆ {1, 0, -1})
  - **Expert 1 (Ï„=0, Î»=1)**: Head expert - Cross-Entropy standard
  - **Expert 2 (Ï„=1, Î»=0)**: Balanced expert - Balanced Softmax  
  - **Expert 3 (Ï„=2, Î»=-1)**: Tail expert - Inverse distribution

### 1.4 Calibration (YÃªu cáº§u báº¯t buá»™c)
- **Method**: Mixup
- **Alpha**: 0.4 (tá»‘i Æ°u cho CIFAR-100-LT)
- **Target mix strategy**: mix_input

## ğŸš€ CÃ¡ch cháº¡y

### PhÆ°Æ¡ng phÃ¡p 1: Sá»­ dá»¥ng script
```bash
# Tá»« thÆ° má»¥c BalPoE gá»‘c
chmod +x moe_plugin_pipeline/run_stage1.sh
./moe_plugin_pipeline/run_stage1.sh
```

### PhÆ°Æ¡ng phÃ¡p 2: Cháº¡y trá»±c tiáº¿p
```bash
# Tá»« thÆ° má»¥c BalPoE gá»‘c
python moe_plugin_pipeline/stage1_train_balpoe.py \
    -c moe_plugin_pipeline/configs/cifar100_ir100_balpoe.json \
    -s 1
```

### PhÆ°Æ¡ng phÃ¡p 3: Sá»­ dá»¥ng train.py gá»‘c vá»›i config
```bash
# Tá»« thÆ° má»¥c BalPoE gá»‘c
python train.py -c moe_plugin_pipeline/configs/cifar100_ir100_balpoe.json -s 1
```

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:

```
checkpoints/balpoe_experts/cifar100_ir100/
â”œâ”€â”€ config.json                    # Config Ä‘Ã£ sá»­ dá»¥ng
â”œâ”€â”€ checkpoint-epoch50.pth         # Checkpoint táº¡i epoch 50
â”œâ”€â”€ checkpoint-epoch100.pth       # Checkpoint táº¡i epoch 100
â”œâ”€â”€ checkpoint-epoch150.pth       # Checkpoint táº¡i epoch 150
â”œâ”€â”€ checkpoint-epoch200.pth       # Checkpoint cuá»‘i cÃ¹ng
â””â”€â”€ model_best.pth               # Model tá»‘t nháº¥t (náº¿u cÃ³)
```

## ğŸ” Kiá»ƒm tra káº¿t quáº£

### 1. Kiá»ƒm tra config Ä‘Ã£ sá»­ dá»¥ng
```bash
cat checkpoints/balpoe_experts/cifar100_ir100/config.json
```

### 2. Kiá»ƒm tra log training
- Logs sáº½ hiá»ƒn thá»‹ trong terminal
- CÃ³ thá»ƒ sá»­ dá»¥ng tensorboard náº¿u Ä‘Æ°á»£c cáº¥u hÃ¬nh

### 3. Kiá»ƒm tra checkpoints
```python
import torch
checkpoint = torch.load('checkpoints/balpoe_experts/cifar100_ir100/checkpoint-epoch200.pth')
print("Keys:", checkpoint.keys())
print("Epoch:", checkpoint['epoch'])
print("Model state dict keys:", list(checkpoint['state_dict'].keys())[:5])
```

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **KhÃ´ng táº¡o hÃ m má»›i**: Stage 1 sá»­ dá»¥ng hoÃ n toÃ n code tá»« BalPoE gá»‘c
2. **Calibration báº¯t buá»™c**: Mixup vá»›i Î±=0.4 lÃ  yÃªu cáº§u lÃ½ thuyáº¿t
3. **Expert diversity**: 3 experts vá»›i Ï„ âˆˆ {0, 1, 2} táº¡o sá»± Ä‘a dáº¡ng tá»‘i Ä‘a
4. **Standard training**: 200 epochs Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng vá»›i BalPoE gá»‘c

## ğŸ› Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **"No module named 'utils'"**
   - Äáº£m báº£o cháº¡y tá»« thÆ° má»¥c BalPoE gá»‘c
   - Kiá»ƒm tra sys.path trong script

2. **"CUDA out of memory"**
   - Giáº£m batch_size trong config
   - Sá»­ dá»¥ng n_gpu: 1 thay vÃ¬ nhiá»u GPU

3. **"Dataset not found"**
   - Äáº£m báº£o cÃ³ thÆ° má»¥c ./data/CIFAR-100
   - Dataset sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº¡o náº¿u chÆ°a cÃ³

## ğŸ“ˆ Monitoring

Trong quÃ¡ trÃ¬nh training, báº¡n sáº½ tháº¥y:
- Training loss cho tá»«ng expert
- Validation accuracy
- Learning rate schedule
- Mixup alpha values

## ğŸ”„ Tiáº¿p theo

Sau khi Stage 1 hoÃ n thÃ nh:
```bash
python moe_plugin_pipeline/stage2_optimize_plugin.py \
    --experts_dir checkpoints/balpoe_experts/cifar100_ir100 \
    --config moe_plugin_pipeline/configs/plugin_optimization.json
```
